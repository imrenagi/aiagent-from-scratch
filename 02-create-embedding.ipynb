{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embedding and Vector Store\n",
    "\n",
    "This notebook demonstrates the process of creating embeddings and setting up a vector store for a course content retrieval system. \n",
    "\n",
    "It covers the following key steps:\n",
    "\n",
    "1. Importing necessary libraries and creating and setting up database and its configurations\n",
    "1. Connecting to either a Google Cloud SQL\n",
    "1. Loading course content data from markdown files\n",
    "1. Creating embeddings for the course content using a Google Gemini embedding model\n",
    "1. Storing the embeddings in a vector database for efficient similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up few constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"imrenagi-gemini-experiment\"  # @param {type:\"string\"}\n",
    "region = \"us-central1\"\n",
    "\n",
    "instance_name=\"pyconapac-demo\"\n",
    "database_password = 'testing' #change this to your database password\n",
    "database_name = 'testing' #change this to your database name\n",
    "database_user = 'testing' #change this to your database user\n",
    "\n",
    "# Dont update these lines below\n",
    "\n",
    "embeddings_table_name = \"course_content_embeddings\"\n",
    "gemini_embedding_model = \"text-embedding-004\"\n",
    "\n",
    "assert database_name, \"⚠️ Please provide a database name\"\n",
    "assert database_user, \"⚠️ Please provide a database user\"\n",
    "assert database_password, \"⚠️ Please provide a database password\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL in Google Cloud SQL\n",
    "\n",
    "Here will we set the default GCP project and get information about the user using the GCP account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure gcloud.\n",
    "!gcloud config set project {project_id}\n",
    "\n",
    "# Grant Cloud SQL Client role to authenticated user\n",
    "current_user = !gcloud auth list --filter=status:ACTIVE --format=\"value(account)\"\n",
    "print(f\"{current_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sending query to database, we will have to add required permissions for our notebook so that it can access the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Granting Cloud SQL Client role to {current_user[0]}\")\n",
    "# granting cloudsql client role to the current user\n",
    "!gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=user:{current_user[0]} \\\n",
    "  --role=\"roles/cloudsql.client\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to create new postgresql database from Google CloudSQL and create postgresql user/role which will be used to store the embeddings later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Create and setup a Cloud SQL PostgreSQL instance, if not done already.\n",
    "database_version = !gcloud sql instances describe {instance_name} --format=\"value(databaseVersion)\"\n",
    "if database_version[0].startswith(\"POSTGRES\"):\n",
    "  print(\"Found an existing Postgres Cloud SQL Instance!\")\n",
    "else:\n",
    "  print(\"Creating new Cloud SQL instance...\")\n",
    "  !gcloud sql instances create {instance_name} --database-version=POSTGRES_15 \\\n",
    "    --region={region} --cpu=1 --memory=4GB --root-password={database_password} \\\n",
    "    --authorized-networks=0.0.0.0/0\n",
    "# Create the database, if it does not exist.\n",
    "out = !gcloud sql databases list --instance={instance_name} --filter=\"NAME:{database_name}\" --format=\"value(NAME)\"\n",
    "if ''.join(out) == database_name:\n",
    "  print(\"Database %s already exists, skipping creation.\" % database_name)\n",
    "else:\n",
    "  !gcloud sql databases create {database_name} --instance={instance_name}\n",
    "# Create the database user for accessing the database.\n",
    "!gcloud sql users create {database_user} \\\n",
    "  --instance={instance_name} \\\n",
    "  --password={database_password}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to get the ip of postgresql we just created. Take note to the database host ip address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ip address of the instance\n",
    "ip_addresses = !gcloud sql instances describe {instance_name} --project {project_id} --format 'value(ipAddresses.ipAddress)'\n",
    "# Split the IP addresses and take the first one\n",
    "database_host = ip_addresses[0].split(';')[0].strip()\n",
    "print(f\"Using database host: {database_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the embeddings\n",
    "\n",
    "Now, we will build the embeddings from the content we have selected. Let's start with storing the content of course_content.jsonl to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSONL file into a pandas DataFrame\n",
    "df = pd.read_json('course_content.jsonl', lines=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the embedding, we need to split the content of each files into chunks. This is most of the time required, especially when the content is toolong, because embedding has the limit for the number of input token it can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "text_splitter = MarkdownTextSplitter(\n",
    "  chunk_size=1000, \n",
    "  chunk_overlap=200)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "chunked = []\n",
    "for index, row in df.iterrows():\n",
    "    course_content_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    content = row[\"content\"]\n",
    "    splits = text_splitter.create_documents([content])\n",
    "    for s in splits:\n",
    "        metadata = {\"course_content_id\": course_content_id, \"title\": title}\n",
    "        doc = Document(page_content=s.page_content, metadata=metadata)\n",
    "        chunked.append(doc)\n",
    "\n",
    "chunked[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the file content chunked into smaller sizes, we are going to create embedding for each chunked and store it to cloudsql.\n",
    "\n",
    "Now let's initialize vertex ai sdk and create the embedding services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=project_id, location=region)\n",
    "# Create a Vertex AI Embeddings service\n",
    "embeddings_service = VertexAIEmbeddings(model_name=gemini_embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct the embeddings and store it to the database.\n",
    "\n",
    "On the function below we are doing these steps:\n",
    "1. We are initiating a PostgresEngine. This instance of PostgresEngine will be used to handle database connection as well as authentication.\n",
    "1. Then, ainit_vectorstore_table() will create a table which will be used to store the chucked content, its embedding, and metadata.\n",
    "1. We initialize the PostgresVectorStore and provide the engine as well as the embedding service.\n",
    "1. For each chunked document, we call function aadd_documents to create embedding and create new record on the given table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresEngine, PostgresVectorStore\n",
    "import uuid\n",
    "\n",
    "async def create_vectorstore():\n",
    "    engine = await PostgresEngine.afrom_instance(\n",
    "        project_id,\n",
    "        region,\n",
    "        instance_name,\n",
    "        database_name,\n",
    "        user=database_user,\n",
    "        password=database_password,\n",
    "    )\n",
    "\n",
    "    await engine.ainit_vectorstore_table(\n",
    "        table_name=embeddings_table_name, vector_size=768, overwrite_existing=True\n",
    "    )\n",
    "\n",
    "    vector_store = await PostgresVectorStore.create(\n",
    "        engine,\n",
    "        table_name=embeddings_table_name,\n",
    "        embedding_service=embeddings_service,\n",
    "    )\n",
    "\n",
    "    ids = [str(uuid.uuid4()) for i in range(len(chunked))]\n",
    "    await vector_store.aadd_documents(chunked, ids=ids)\n",
    "\n",
    "await create_vectorstore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
