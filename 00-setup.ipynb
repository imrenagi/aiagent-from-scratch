{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI Agent Bot With RAG, Langchain, and Reasoning Engine From Scratch\n",
    "\n",
    "## Setup\n",
    "\n",
    "* This notebook will walk you through some required setup that you need to do before starting with the materials.\n",
    "\n",
    "* It is highly recommended to use new virtual environment when running jupyter notebook for this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Software Installed Locally\n",
    "\n",
    "* Python version 3.9, 3.10, or 3.11. **Python3.12 will not work**.\n",
    "\n",
    "* If you are using VSCode, please install Jupyter Notebook extensions.\n",
    "\n",
    "* Jupyter notebook. Please follow this [installation guide](https://docs.jupyter.org/en/stable/install.html). You may choose whether you want to install classic jupyter notebook or jupyterlab (the next-gen web ui for jupyter)\n",
    "\n",
    "    * [Classic jupyter notebook installation guide](https://docs.jupyter.org/en/stable/install/notebook-classic.html)\n",
    "\n",
    "    * [Jupyterlab installation guide](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html)\n",
    "\n",
    "* Google Cloud CLI. Please follow this [installation guide](https://cloud.google.com/sdk/docs/install-sdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "google-cloud-aiplatform\n",
    "google-cloud-aiplatform[langchain]\n",
    "google-cloud-aiplatform[reasoningengine]\n",
    "langchain\n",
    "langchain_core\n",
    "langchain_community\n",
    "langchain-google-vertexai==2.0.8\n",
    "cloudpickle\n",
    "pydantic==2.9.2\n",
    "langchain-google-community\n",
    "google-cloud-discoveryengine\n",
    "nest-asyncio\n",
    "asyncio==3.4.3\n",
    "asyncpg==0.29.0\n",
    "cloud-sql-python-connector[asyncpg]\n",
    "langchain-google-cloud-sql-pg\n",
    "numpy\n",
    "pandas\n",
    "pgvector\n",
    "psycopg2-binary\n",
    "langchain-openai\n",
    "langgraph\n",
    "traceloop-sdk\n",
    "opentelemetry-instrumentation-google-generativeai\n",
    "opentelemetry-instrumentation-langchain\n",
    "opentelemetry-instrumentation-vertexai\n",
    "python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in case you are facing issue with installing psycopg2, please run the following command (linux only):\n",
    "\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt install python3-dev libpq-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will require to restart the jupyter kernel once the dependency installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Google Cloud Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommended account setup\n",
    "\n",
    "if you are running this in jupyter notebook locally, you may need to login to google cloud by running the following command from terminal:\n",
    "\n",
    "```\n",
    "gcloud auth login\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colabs, you need to authenticate with your google account by running the following notebook cell. \n",
    "\n",
    "> Please remember that you will need to do this on each jupyter notebook during this workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@markdown ###Authenticate your Google Cloud Account and enable APIs.\n",
    "# # Authenticate gcloud.\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Google Cloud Credit\n",
    "\n",
    "Please redeem your $5 USD credit that you can use for this workshop. Link for this, will be shared on the class room.\n",
    "\n",
    "The instruction given will also require you to create a new GCP project. Create one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Google Service API\n",
    "\n",
    "Before creating cloud resources (e.g. database, cloudrun services, reasoning engine, etc), first we must enable the services api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Replace the required placeholder text below. You can modify any other default values, if you like.\n",
    "\n",
    "# please change the project id into your gcp project id you just created. \n",
    "project_id = \"imrenagi-gemini-experiment\"  # @param {type:\"string\"}\n",
    "\n",
    "# you can leave this the same.\n",
    "region = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "!gcloud config set project {project_id} --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "service = discovery.build(\"cloudresourcemanager\", \"v1\")\n",
    "request = service.projects().get(projectId=project_id)\n",
    "response = request.execute()\n",
    "project_number = response[\"projectNumber\"]\n",
    "project_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will enable few services:\n",
    "\n",
    "* `aiplatform.googleapis.com` -> used for using Gemini LLM and reasoning engine\n",
    "* `run.googleapis.com` -> used for deploying to cloud run\n",
    "* `cloudbuild.googleapis.com` -> used for building docker image and perform the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable run.googleapis.com \n",
    "!gcloud services enable cloudbuild.googleapis.com\n",
    "!gcloud services enable sqladmin.googleapis.com\n",
    "!gcloud services enable cloudtrace.googleapis.com\n",
    "\n",
    "!gcloud beta services identity create --service=aiplatform.googleapis.com --project={project_id}\n",
    "\n",
    "!gcloud projects add-iam-policy-binding {project_id} \\\n",
    "    --member=serviceAccount:{project_number}-compute@developer.gserviceaccount.com \\\n",
    "    --role=\"roles/cloudbuild.builds.builder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Dummy API server\n",
    "\n",
    "Later on this workshop, you will be using your AI agent to interact with api in order to get detail about an online course you provide as well as to create purchase request. Hence, we will deploy the simple stupid API to cloudrun.\n",
    "\n",
    "If you want to see the detail, you can check the `api/` directory.\n",
    "\n",
    "Now let's deploy the Go API to cloud run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this registry name with an unique name\n",
    "registry_name = \"imrenagi-gemini-experiment-registry\"  # @param {type:\"string\"}\n",
    "\n",
    "!gcloud artifacts repositories create {registry_name} \\\n",
    "      --repository-format=docker \\\n",
    "      --location={region} \\\n",
    "      --description=\"devfest artifact registry\" \\\n",
    "      --immutable-tags       \n",
    "\n",
    "registry_url = f\"{region}-docker.pkg.dev/{project_id}/{registry_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the docker image used by the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit api --tag {registry_url}/courses-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy the docker image to cloud run so that we can have the api up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud run deploy courses-api --allow-unauthenticated --region {region} --quiet --image {registry_url}/courses-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it is deployed, run the command to get the url of your dummy api. Take note because we will use it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = !gcloud run services describe courses-api --region=us-central1 --format='value(status.url)'\n",
    "api_url = urls[0]\n",
    "print(api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl {api_url}/courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Staging Bucket for AI Agent\n",
    "\n",
    "Later, when we deploy the AI Agent, we have to provide the staging gcs bucket used to store the pickle and some other configurations of our reasoning engine. So, let's create a new empty bucket. Please change `staging_bucket_name` variable below with globally unique name.\n",
    "\n",
    "Once the bucket created, take note the name of the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this with globaly unique name. you may add your name to make it unique. this bucket will be used later for storing the model\n",
    "staging_bucket_name = \"ai-agent-demo-bucket\" # @param {type:\"string\"}\n",
    "\n",
    "!gcloud storage buckets create gs://{staging_bucket_name} --project={project_id} --location={region} --uniform-bucket-level-access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this workshop, we are going to use written content from [OWASP CheatSheetSeries](https://github.com/OWASP/CheatSheetSeries) as the source document for our RAG. However, to reduce the cost, I already currated few files that we are going to use in `urls` variable. Instead of using all of them, we will just use few of them and build embedding with the currated files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code below will just iterate over all files within `sources` directory and create a `course_content.jsonl` file containing the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/OWASP/CheatSheetSeries/refs/heads/master/cheatsheets/Authentication_Cheat_Sheet.md\",\n",
    "    \"https://raw.githubusercontent.com/OWASP/CheatSheetSeries/refs/heads/master/cheatsheets/Authorization_Cheat_Sheet.md\",\n",
    "    \"https://raw.githubusercontent.com/OWASP/CheatSheetSeries/refs/heads/master/cheatsheets/File_Upload_Cheat_Sheet.md\",\n",
    "    \"https://raw.githubusercontent.com/OWASP/CheatSheetSeries/refs/heads/master/cheatsheets/Forgot_Password_Cheat_Sheet.md\",\n",
    "    \"https://raw.githubusercontent.com/OWASP/CheatSheetSeries/refs/heads/master/cheatsheets/Password_Storage_Cheat_Sheet.md\",\n",
    "    \"https://raw.githubusercontent.com/OWASP/CheatSheetSeries/refs/heads/master/cheatsheets/REST_Security_Cheat_Sheet.md\",\n",
    "    \"https://raw.githubusercontent.com/OWASP/CheatSheetSeries/refs/heads/master/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.md\"\n",
    "]\n",
    "\n",
    "def generate_course_content_jsonl():\n",
    "    output_file = 'course_content.jsonl'\n",
    "    \n",
    "    with open(output_file, 'w') as jsonl_file:\n",
    "\n",
    "        for url in urls:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text\n",
    "                filename = url.split('/')[-1]                \n",
    "                title = filename.replace('_', ' ').replace('.md', '')\n",
    "\n",
    "                slug = title.lower().replace(' ', '-')\n",
    "                            \n",
    "                record = {\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'title': title,\n",
    "                    'content': content,\n",
    "                    'file_path': str(url),\n",
    "                    'slug': slug\n",
    "                }                \n",
    "                json.dump(record, jsonl_file)\n",
    "                jsonl_file.write('\\n')\n",
    "            else:\n",
    "                print(f\"Failed to download content. Status code: {response.status_code}\")\n",
    "\n",
    "        \n",
    "    print(f\"JSONL file '{output_file}' has been generated successfully.\")\n",
    "\n",
    "generate_course_content_jsonl()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is inside the `course_content.jsonl` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('course_content.jsonl', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embedding and Vector Store\n",
    "\n",
    "This notebook demonstrates the process of creating embeddings and setting up a vector store for a course content retrieval system. \n",
    "\n",
    "It covers the following key steps:\n",
    "\n",
    "1. Importing necessary libraries and creating and setting up database and its configurations\n",
    "1. Connecting to either a Google Cloud SQL\n",
    "1. Loading course content data from markdown files\n",
    "1. Creating embeddings for the course content using a Google Gemini embedding model\n",
    "1. Storing the embeddings in a vector database for efficient similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up few constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "instance_name = os.environ['CLOUDSQL_INSTANCE_NAME']\n",
    "database_password = os.environ['DB_PASSWORD']\n",
    "database_name = os.environ['DB_NAME']\n",
    "database_user = os.environ['DB_USER']\n",
    "\n",
    "# Dont update these lines below\n",
    "\n",
    "embeddings_table_name = \"course_content_embeddings\"\n",
    "chat_history_table_name = \"chat_histories\"\n",
    "gemini_embedding_model = \"text-embedding-004\"\n",
    "\n",
    "assert database_name, \"⚠️ Please provide a database name\"\n",
    "assert database_user, \"⚠️ Please provide a database user\"\n",
    "assert database_password, \"⚠️ Please provide a database password\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL in Google Cloud SQL\n",
    "\n",
    "Here will we set the default GCP project and get information about the user using the GCP account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grant Cloud SQL Client role to authenticated user\n",
    "current_user = !gcloud auth list --filter=status:ACTIVE --format=\"value(account)\"\n",
    "print(f\"{current_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sending query to database, we will have to add required permissions for our notebook so that it can access the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Granting Cloud SQL Client role to {current_user[0]}\")\n",
    "# granting cloudsql client role to the current user\n",
    "!gcloud projects add-iam-policy-binding {project_id} \\\n",
    "  --member=user:{current_user[0]} \\\n",
    "  --role=\"roles/cloudsql.client\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to create new postgresql database from Google CloudSQL and create postgresql user/role which will be used to store the embeddings later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Create and setup a Cloud SQL PostgreSQL instance, if not done already.\n",
    "database_version = !gcloud sql instances describe {instance_name} --format=\"value(databaseVersion)\"\n",
    "if database_version[0].startswith(\"POSTGRES\"):\n",
    "  print(\"Found an existing Postgres Cloud SQL Instance!\")\n",
    "else:\n",
    "  print(\"Creating new Cloud SQL instance...\")\n",
    "  !gcloud sql instances create {instance_name} --database-version=POSTGRES_15 \\\n",
    "    --region={region} --cpu=1 --memory=4GB --root-password={database_password} \\\n",
    "    --authorized-networks=0.0.0.0/0\n",
    "# Create the database, if it does not exist.\n",
    "out = !gcloud sql databases list --instance={instance_name} --filter=\"NAME:{database_name}\" --format=\"value(NAME)\"\n",
    "if ''.join(out) == database_name:\n",
    "  print(\"Database %s already exists, skipping creation.\" % database_name)\n",
    "else:\n",
    "  !gcloud sql databases create {database_name} --instance={instance_name}\n",
    "# Create the database user for accessing the database.\n",
    "!gcloud sql users create {database_user} \\\n",
    "  --instance={instance_name} \\\n",
    "  --password={database_password}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to get the ip of postgresql we just created. Take note to the database host ip address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ip address of the instance\n",
    "ip_addresses = !gcloud sql instances describe {instance_name} --project {project_id} --format 'value(ipAddresses.ipAddress)'\n",
    "# Split the IP addresses and take the first one\n",
    "database_host = ip_addresses[0].split(';')[0].strip()\n",
    "print(f\"Using database host: {database_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the embeddings\n",
    "\n",
    "Now, we will build the embeddings from the content we have selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the embedding, we need to split the content of each files into chunks. This is most of the time required, especially when the content is toolong, because embedding has the limit for the number of input token it can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "text_splitter = MarkdownTextSplitter(\n",
    "  chunk_size=1000, \n",
    "  chunk_overlap=200)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "chunked = []\n",
    "for index, row in df.iterrows():\n",
    "    course_content_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    content = row[\"content\"]\n",
    "    splits = text_splitter.create_documents([content])\n",
    "    for s in splits:\n",
    "        metadata = {\"course_content_id\": course_content_id, \"title\": title}\n",
    "        doc = Document(page_content=s.page_content, metadata=metadata)\n",
    "        chunked.append(doc)\n",
    "\n",
    "chunked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the file content chunked into smaller sizes, we are going to create embedding for each chunked and store it to cloudsql.\n",
    "\n",
    "Now let's initialize vertex ai sdk and create the embedding services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "import vertexai\n",
    "\n",
    "print(project_id)\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=project_id, location=region)\n",
    "# Create a Vertex AI Embeddings service\n",
    "embeddings_service = VertexAIEmbeddings(model_name=gemini_embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct the embeddings and store it to the database.\n",
    "\n",
    "On the function below we are doing these steps:\n",
    "1. We are initiating a PostgresEngine. This instance of PostgresEngine will be used to handle database connection as well as authentication.\n",
    "1. Then, `ainit_vectorstore_table()` will create a table which will be used to store the chucked content, its embedding, and metadata.\n",
    "1. We initialize the PostgresVectorStore and provide the engine as well as the embedding service.\n",
    "1. For each chunked document, we call function `aadd_documents` to create embedding and create new record on the given table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip-system-certs\n",
    "!pip install --upgrade certifi urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresEngine, PostgresVectorStore\n",
    "import uuid\n",
    "\n",
    "async def create_vectorstore():\n",
    "    engine = await PostgresEngine.afrom_instance(\n",
    "        project_id,\n",
    "        region,\n",
    "        instance_name,\n",
    "        database_name,\n",
    "        user=database_user,\n",
    "        password=database_password,\n",
    "    )\n",
    "\n",
    "    await engine.ainit_chat_history_table(\n",
    "        table_name=chat_history_table_name\n",
    "    )\n",
    "\n",
    "    await engine.ainit_vectorstore_table(\n",
    "        table_name=embeddings_table_name, vector_size=768, overwrite_existing=True\n",
    "    )\n",
    "\n",
    "    vector_store = await PostgresVectorStore.create(\n",
    "        engine,\n",
    "        table_name=embeddings_table_name,\n",
    "        embedding_service=embeddings_service,\n",
    "    )\n",
    "\n",
    "    ids = [str(uuid.uuid4()) for i in range(len(chunked))]\n",
    "    await vector_store.aadd_documents(chunked, ids=ids)\n",
    "\n",
    "await create_vectorstore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the vector store, you can check the content from google cloud sql data viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever\n",
    "\n",
    "Once we have data stored in cloudsql, we need to find a way to query the data. This notebook covers how we can create and use the postgresql retriever to perform similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to previous section, we will try to create PostgresEngine to connect to CloudSQL instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresEngine\n",
    "\n",
    "pg_engine = PostgresEngine.from_instance(\n",
    "    project_id=project_id,\n",
    "    instance=instance_name,\n",
    "    region=region,\n",
    "    database=database_name,\n",
    "    user=database_password,\n",
    "    password=database_password,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the vector store object by using the engine and embedding service we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_cloud_sql_pg import PostgresVectorStore\n",
    "\n",
    "vector_store = PostgresVectorStore.create_sync(\n",
    "            pg_engine,\n",
    "            table_name=embeddings_table_name,\n",
    "            embedding_service=embeddings_service,\n",
    "        )\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with some query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"how to design forgot password?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"how to design security for authentication?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
